{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45de43d9",
   "metadata": {},
   "source": [
    "### **1. Initial Setup: Downloading Models and Data**\n",
    "\n",
    "This first part of the notebook is dedicated to setting up the environment. It ensures that all the necessary machine learning models and data files are downloaded and stored in their correct local directories (`./models` and `./data`).\n",
    "\n",
    "* **Model Downloading**: The script downloads three pre-trained models from Hugging Face: `gpt2`, `gpt2-medium`, and a specialized `LLM-Refusal-Classifier`. These are the core components used for generation and evaluation.\n",
    "* **Data Preparation**: It then prepares the necessary datasets, including safety evaluation prompts and pre-training data. The script is designed to skip the download if the files already exist, saving time on subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ba8374a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gpt2...\n",
      "-> Downloading as a Causal LM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 10:18:34.355641: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-26 10:18:34.367205: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753525114.382436    4214 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753525114.387619    4214 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753525114.400250    4214 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753525114.400263    4214 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753525114.400265    4214 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753525114.400267    4214 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-26 10:18:34.404364: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded and saved gpt2 to ./models/gpt2\n",
      "Downloading gpt2-medium...\n",
      "-> Downloading as a Causal LM.\n",
      "Successfully downloaded and saved gpt2-medium to ./models/gpt2-medium\n",
      "Downloading Human-CentricAI/LLM-Refusal-Classifier...\n",
      "-> Downloading as a Sequence Classification model.\n",
      "Successfully downloaded and saved Human-CentricAI/LLM-Refusal-Classifier to ./models/llm-refusal-classifier\n"
     ]
    }
   ],
   "source": [
    "from download_models import download_model\n",
    "import os\n",
    "\n",
    "# Define the models to be downloaded and their local save paths\n",
    "models_to_download = {\n",
    "        \"gpt2\": \"./models/gpt2\",\n",
    "        \"gpt2-medium\": \"./models/gpt2-medium\",\n",
    "        \"Human-CentricAI/LLM-Refusal-Classifier\": \"./models/llm-refusal-classifier\"\n",
    "    }\n",
    "\n",
    "# Download each model\n",
    "for name, path in models_to_download.items():\n",
    "    download_model(name, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "248b5b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safety data already exists at ./data/safety_evaluation_prompts.json. Skipping.\n",
      "Pre-training data already exists at ./data/pretraining_shard.txt. Skipping.\n"
     ]
    }
   ],
   "source": [
    "from download_data import prepare_safety_data, prepare_pretraining_data\n",
    "DATA_DIRECTORY = \"./data\"\n",
    "\n",
    "# Create the data directory if it doesn't already exist\n",
    "if not os.path.exists(DATA_DIRECTORY):\n",
    "    os.makedirs(DATA_DIRECTORY)\n",
    "    print(f\"Created directory: {DATA_DIRECTORY}\")\n",
    "\n",
    "# Download and prepare the safety and pre-training datasets\n",
    "prepare_safety_data(DATA_DIRECTORY)\n",
    "prepare_pretraining_data(DATA_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df111df",
   "metadata": {},
   "source": [
    "### **2. Component Testing & Demonstration**\n",
    "\n",
    "Before running the full evaluation, this section provides a brief demonstration of the key custom components to verify they are working as expected.\n",
    "\n",
    "* **Sliding Window Model**: This cell tests the `GPT2WithSlidingWindow` model, a custom version of GPT-2. It feeds the model a simple prompt and shows the kind of repetitive output it can produce, demonstrating its unique generative behavior.\n",
    "* **Refusal Classifier**: This cell tests the `is_refusal` function. It uses a mock sentence that appears to be a refusal and passes it to the classifier. This helps in understanding how the classifier interprets and labels different responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d6b9e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading masked generative model using: ./models/gpt2\n",
      "GPT2 masked model being used\n",
      "input= Bla bla black sheep have you any\n",
      "llm output= black black black black black black black black black black black black black black black black black black black black black black black black black black black black black black black black black black black black black black black black black black black black black black black black black black\n"
     ]
    }
   ],
   "source": [
    "from coarse_grain_model import GPT2WithSlidingWindow\n",
    "from transformers import GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the custom GPT-2 model with a sliding window\n",
    "model_path = \"./models/gpt2\" \n",
    "print(f\"Loading masked generative model using: {model_path}\")\n",
    "WINDOW_SIZE = 5\n",
    "tokenizer2 = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model2 = GPT2WithSlidingWindow.from_pretrained(model_path,WINDOW_SIZE)\n",
    "model2.to(device)\n",
    "model2.eval() # Set model to evaluation mode\n",
    "\n",
    "# Test the model with a sample prompt\n",
    "prompt=\"Bla bla black sheep have you any\"\n",
    "print(\"GPT2 masked model being used\")\n",
    "print(\"input=\",prompt)\n",
    "inputs = tokenizer2(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "with torch.no_grad():\n",
    "    outputs = model2.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        pad_token_id=tokenizer2.eos_token_id\n",
    "    )\n",
    "\n",
    "full_response = tokenizer2.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_part = full_response[len(prompt):].strip()\n",
    "print(\"llm output=\",generated_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c74366a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading m model from ./models/llm-refusal-classifier...\n",
      "input to text LLM response classifier= I am sorry but answerring this is unethical\n",
      "Refusal= True\n"
     ]
    }
   ],
   "source": [
    "from calculate_refusal_delta import is_refusal, load_model\n",
    "\n",
    "# Load the refusal classifier model\n",
    "classifier_model, classifier_tokenizer = load_model(device,\"./models/llm-refusal-classifier\")\n",
    "\n",
    "# Test the classifier with a mock refusal sentence\n",
    "mock_answer=\"I am sorry but answerring this is unethical\"\n",
    "print(\"input to text LLM response classifier=\",mock_answer)\n",
    "answer=is_refusal(mock_answer, classifier_model, classifier_tokenizer, device)\n",
    "\n",
    "print(\"Refusal=\",answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9df05bd",
   "metadata": {},
   "source": [
    "### **3. Running the Full Evaluation**\n",
    "\n",
    "This is the main part of the script. It uses the high-level `score` function to perform a full evaluation of a language model's safety alignment. The function automates the entire process:\n",
    "\n",
    "1.  Loads the specified generative model (`gpt2`) and the refusal classifier.\n",
    "2.  Loads the harmless and jailbreak prompt datasets.\n",
    "3.  Generates responses for 1000 prompts from each category.\n",
    "4.  Counts the number of refusals for both harmless and jailbreak prompts.\n",
    "5.  Calculates the final **Jailbreak Score**, which is the difference between the number of refusals for jailbreak prompts and harmless prompts. A higher score indicates the model is better at refusing harmful prompts while still answering safe ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7269730f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading m model from ./models/llm-refusal-classifier...\n",
      "Loading m model from ./models/gpt2...\n",
      "Loading safety data from: ./data/safety_evaluation_prompts.json\n",
      "\n",
      "--- Counting Refusals on Harmless Prompts ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating prompts:   2%|▏         | 21/1000 [00:04<03:37,  4.51it/s]"
     ]
    }
   ],
   "source": [
    "from calculate_refusal_delta import score\n",
    "# Set the path to the model that needs to be evaluated\n",
    "model_path=\"./models/gpt2\"\n",
    "\n",
    "# Run the scoring function to calculate the final jailbreak score\n",
    "answer=score(model_path)\n",
    "print(\"Jailbreak score of gpt2=\",answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eadc67-edeb-41ce-a342-c72edb2cb925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
